{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "a194102854a9eaec85445c81d0621d55f1476560"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 128\n",
    "pd.options.display.max_columns = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6911a2be5a3b016869b7a94c16bfeb82cd0a7342"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f16a86529d54b5fbc3d24569da81077abab9ba21"
   },
   "source": [
    "### load core DFs (train and test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "8473644b13e590420cdc349f7fb27a541fb6644b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.csv', 'sample_submission.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../input/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "54985b02f752cd86dd2a39ac261281573ca9e81d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train/train.csv')\n",
    "test = pd.read_csv('../input/test/test.csv')\n",
    "sample_submission = pd.read_csv('../input/test/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43025d0dbd7886a7e3d7168a45bec6790307ff44"
   },
   "source": [
    "### load mapping dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "cdb36fcca79a9c8db7f139d7266d8c8e4ad021f3"
   },
   "outputs": [],
   "source": [
    "labels_breed = pd.read_csv('../input/breed_labels.csv')\n",
    "labels_state = pd.read_csv('../input/color_labels.csv')\n",
    "labels_color = pd.read_csv('../input/state_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd40d640362e85d9818b98e51b0be3aec443762b"
   },
   "source": [
    "### additional data:\n",
    "\n",
    "We have also additional information about pets available in form of:\n",
    "\n",
    "- images\n",
    "- metadata\n",
    "- sentiment\n",
    "\n",
    "Integration of those will enable us to possibly improve the score.\n",
    "Information derived from example from images should be very important, as picture of a pet influences the way we look at an animal in a significant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "876557740f8c8ccc2b4c0080acee005fb2eff73f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train images files: 58311\n",
      "num of train metadata files: 58311\n",
      "num of train sentiment files: 14442\n",
      "num of test images files: 15040\n",
      "num of test metadata files: 15040\n",
      "num of test sentiment files: 3815\n"
     ]
    }
   ],
   "source": [
    "train_image_files = sorted(glob.glob('../input/train_images/*.jpg'))\n",
    "train_metadata_files = sorted(glob.glob('../input/train_metadata/*.json'))\n",
    "train_sentiment_files = sorted(glob.glob('../input/train_sentiment/*.json'))\n",
    "\n",
    "print('num of train images files: {}'.format(len(train_image_files)))\n",
    "print('num of train metadata files: {}'.format(len(train_metadata_files)))\n",
    "print('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n",
    "\n",
    "\n",
    "test_image_files = sorted(glob.glob('../input/test_images/*.jpg'))\n",
    "test_metadata_files = sorted(glob.glob('../input/test_metadata/*.json'))\n",
    "test_sentiment_files = sorted(glob.glob('../input/test_sentiment/*.json'))\n",
    "\n",
    "print('num of test images files: {}'.format(len(test_image_files)))\n",
    "print('num of test metadata files: {}'.format(len(test_metadata_files)))\n",
    "print('num of test sentiment files: {}'.format(len(test_sentiment_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f0faa84d0f6382e59e704680a0291687022c99e"
   },
   "source": [
    "### train analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "96053ab161520c2d8d40a60e0a2fdf649f65bb23",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 1)\n",
      "14652\n",
      "fraction of pets with images: 0.977\n",
      "14652\n",
      "fraction of pets with metadata: 0.977\n",
      "14442\n",
      "fraction of pets with sentiment: 0.963\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# Images:\n",
    "train_df_ids = train[['PetID']]\n",
    "print(train_df_ids.shape)\n",
    "\n",
    "train_df_imgs = pd.DataFrame(train_image_files)\n",
    "train_df_imgs.columns = ['image_filename']\n",
    "train_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "train_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\n",
    "print(len(train_imgs_pets.unique()))\n",
    "\n",
    "pets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with images: {:.3f}'.format(pets_with_images / train_df_ids.shape[0]))\n",
    "\n",
    "# Metadata:\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_metadata = pd.DataFrame(train_metadata_files)\n",
    "train_df_metadata.columns = ['metadata_filename']\n",
    "train_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "train_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\n",
    "print(len(train_metadata_pets.unique()))\n",
    "\n",
    "pets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / train_df_ids.shape[0]))\n",
    "\n",
    "# Sentiment:\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_sentiment = pd.DataFrame(train_sentiment_files)\n",
    "train_df_sentiment.columns = ['sentiment_filename']\n",
    "train_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "train_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\n",
    "print(len(train_sentiment_pets.unique()))\n",
    "\n",
    "pets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / train_df_ids.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "2d6e48743d5bc42015320b367cc6e11393233264",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3948, 1)\n",
      "3821\n",
      "fraction of pets with images: 0.968\n",
      "3821\n",
      "fraction of pets with metadata: 0.968\n",
      "3815\n",
      "fraction of pets with sentiment: 0.966\n",
      "images and metadata distributions the same? True\n"
     ]
    }
   ],
   "source": [
    "# Images:\n",
    "test_df_ids = test[['PetID']]\n",
    "print(test_df_ids.shape)\n",
    "\n",
    "test_df_imgs = pd.DataFrame(test_image_files)\n",
    "test_df_imgs.columns = ['image_filename']\n",
    "test_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "test_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n",
    "print(len(test_imgs_pets.unique()))\n",
    "\n",
    "pets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with images: {:.3f}'.format(pets_with_images / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# Metadata:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_metadata = pd.DataFrame(test_metadata_files)\n",
    "test_df_metadata.columns = ['metadata_filename']\n",
    "test_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "test_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\n",
    "print(len(test_metadata_pets.unique()))\n",
    "\n",
    "pets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "# Sentiment:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_sentiment = pd.DataFrame(test_sentiment_files)\n",
    "test_df_sentiment.columns = ['sentiment_filename']\n",
    "test_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "test_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\n",
    "print(len(test_sentiment_pets.unique()))\n",
    "\n",
    "pets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# are distributions the same?\n",
    "print('images and metadata distributions the same? {}'.format(\n",
    "    np.all(test_metadata_pets == test_imgs_pets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18f435d349d1fc384c645b41233d2c5a22860903"
   },
   "source": [
    "### data parsing & feature extraction:\n",
    "\n",
    "After taking a look at the data, we know its structure and can use it to extract additional features and concatenate them with basic train/test DFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "1db107527d3ef5ba55560679334d16f1d82fb663"
   },
   "outputs": [],
   "source": [
    "class PetFinderParser(object):\n",
    "    \n",
    "    def __init__(self, debug=False):\n",
    "        \n",
    "        self.debug = debug\n",
    "        self.sentence_sep = ' '\n",
    "        \n",
    "        # Does not have to be extracted because main DF already contains description\n",
    "        self.extract_sentiment_text = False\n",
    "        \n",
    "        \n",
    "    def open_metadata_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load metadata file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            metadata_file = json.load(f)\n",
    "        return metadata_file\n",
    "            \n",
    "    def open_sentiment_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load sentiment file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            sentiment_file = json.load(f)\n",
    "        return sentiment_file\n",
    "            \n",
    "    def open_image_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load image file.\n",
    "        \"\"\"\n",
    "        image = np.asarray(Image.open(filename))\n",
    "        return image\n",
    "        \n",
    "    def parse_sentiment_file(self, file):\n",
    "        \"\"\"\n",
    "        Parse sentiment file. Output DF with sentiment features.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_sentiment = file['documentSentiment']\n",
    "        file_entities = [x['name'] for x in file['entities']]\n",
    "        file_entities = self.sentence_sep.join(file_entities)\n",
    "\n",
    "        if self.extract_sentiment_text:\n",
    "            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n",
    "            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n",
    "        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n",
    "        \n",
    "        file_sentences_sentiment = pd.DataFrame.from_dict(\n",
    "            file_sentences_sentiment, orient='columns').sum()\n",
    "        file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n",
    "        \n",
    "        file_sentiment.update(file_sentences_sentiment)\n",
    "        \n",
    "        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n",
    "        if self.extract_sentiment_text:\n",
    "            df_sentiment['text'] = file_sentences_text\n",
    "            \n",
    "        df_sentiment['entities'] = file_entities\n",
    "        df_sentiment = df_sentiment.add_prefix('sentiment_')\n",
    "        \n",
    "        return df_sentiment\n",
    "    \n",
    "    def parse_metadata_file(self, file):\n",
    "        \"\"\"\n",
    "        Parse metadata file. Output DF with metadata features.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_keys = list(file.keys())\n",
    "        \n",
    "        if 'labelAnnotations' in file_keys:\n",
    "            file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n",
    "            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n",
    "            file_top_desc = [x['description'] for x in file_annots]\n",
    "        else:\n",
    "            file_top_score = np.nan\n",
    "            file_top_desc = ['']\n",
    "        \n",
    "        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "        file_crops = file['cropHintsAnnotation']['cropHints']\n",
    "\n",
    "        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "\n",
    "        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n",
    "        \n",
    "        if 'importanceFraction' in file_crops[0].keys():\n",
    "            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n",
    "        else:\n",
    "            file_crop_importance = np.nan\n",
    "\n",
    "        df_metadata = {\n",
    "            'annots_score': file_top_score,\n",
    "            'color_score': file_color_score,\n",
    "            'color_pixelfrac': file_color_pixelfrac,\n",
    "            'crop_conf': file_crop_conf,\n",
    "            'crop_importance': file_crop_importance,\n",
    "            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n",
    "        }\n",
    "        \n",
    "        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n",
    "        df_metadata = df_metadata.add_prefix('metadata_')\n",
    "        \n",
    "        return df_metadata\n",
    "    \n",
    "\n",
    "# Helper function for parallel data processing:\n",
    "def extract_additional_features(pet_id, mode='train'):\n",
    "    \n",
    "    sentiment_filename = '../input/{}_sentiment/{}.json'.format(mode, pet_id)\n",
    "    try:\n",
    "        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n",
    "        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n",
    "        df_sentiment['PetID'] = pet_id\n",
    "    except FileNotFoundError:\n",
    "        df_sentiment = []\n",
    "\n",
    "    dfs_metadata = []\n",
    "    metadata_filenames = sorted(glob.glob('../input/{}_metadata/{}*.json'.format(mode, pet_id)))\n",
    "    if len(metadata_filenames) > 0:\n",
    "        for f in metadata_filenames:\n",
    "            metadata_file = pet_parser.open_metadata_file(f)\n",
    "            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n",
    "            df_metadata['PetID'] = pet_id\n",
    "            dfs_metadata.append(df_metadata)\n",
    "        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n",
    "    dfs = [df_sentiment, dfs_metadata]\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "\n",
    "pet_parser = PetFinderParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "1b8c6eedb089ac6df143592de6455c1978b460f6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=6)]: Done 1238 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=6)]: Done 1788 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=6)]: Done 2438 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=6)]: Done 3188 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=6)]: Done 4038 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=6)]: Done 4988 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=6)]: Done 6038 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=6)]: Done 7188 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=6)]: Done 8438 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=6)]: Done 9788 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=6)]: Done 11238 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=6)]: Done 12788 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=6)]: Done 14438 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=6)]: Done 14993 out of 14993 | elapsed: 10.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14442, 6) (58311, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=6)]: Done 716 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=6)]: Done 1716 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=6)]: Done 3116 tasks      | elapsed:   46.6s\n",
      "[Parallel(n_jobs=6)]: Done 3948 out of 3948 | elapsed:   58.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3815, 6) (15040, 7)\n"
     ]
    }
   ],
   "source": [
    "# Unique IDs from train and test:\n",
    "debug = False\n",
    "train_pet_ids = train.PetID.unique()\n",
    "test_pet_ids = test.PetID.unique()\n",
    "\n",
    "if debug:\n",
    "    train_pet_ids = train_pet_ids[:1000]\n",
    "    test_pet_ids = test_pet_ids[:500]\n",
    "\n",
    "\n",
    "# Train set:\n",
    "# Parallel processing of data:\n",
    "dfs_train = Parallel(n_jobs=6, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n",
    "\n",
    "# Extract processed data and format them as DFs:\n",
    "train_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\n",
    "train_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "train_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\n",
    "train_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "print(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n",
    "\n",
    "\n",
    "# Test set:\n",
    "# Parallel processing of data:\n",
    "dfs_test = Parallel(n_jobs=6, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n",
    "\n",
    "# Extract processed data and format them as DFs:\n",
    "test_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\n",
    "test_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "test_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\n",
    "test_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "print(test_dfs_sentiment.shape, test_dfs_metadata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "76489c2957389009c1b55ec2142ebf5dd9d7a923"
   },
   "source": [
    "### group extracted features by PetID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "558824a494ce6b6c5821d2cb85420393bf728338"
   },
   "outputs": [],
   "source": [
    "# Extend aggregates and improve column naming\n",
    "aggregates = ['mean', 'sum']\n",
    "\n",
    "\n",
    "# Train\n",
    "train_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "train_metadata_desc = train_metadata_desc.reset_index()\n",
    "train_metadata_desc[\n",
    "    'metadata_annots_top_desc'] = train_metadata_desc[\n",
    "    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'metadata'\n",
    "train_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\n",
    "for i in train_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\n",
    "train_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "train_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\n",
    "train_metadata_gr = train_metadata_gr.reset_index()\n",
    "\n",
    "\n",
    "train_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "train_sentiment_desc = train_sentiment_desc.reset_index()\n",
    "train_sentiment_desc[\n",
    "    'sentiment_entities'] = train_sentiment_desc[\n",
    "    'sentiment_entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'sentiment'\n",
    "train_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\n",
    "for i in train_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\n",
    "train_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates)\n",
    "train_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\n",
    "train_sentiment_gr = train_sentiment_gr.reset_index()\n",
    "\n",
    "\n",
    "# Test\n",
    "test_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "test_metadata_desc = test_metadata_desc.reset_index()\n",
    "test_metadata_desc[\n",
    "    'metadata_annots_top_desc'] = test_metadata_desc[\n",
    "    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'metadata'\n",
    "test_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\n",
    "for i in test_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\n",
    "test_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "test_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in test_metadata_gr.columns.tolist()])\n",
    "test_metadata_gr = test_metadata_gr.reset_index()\n",
    "\n",
    "\n",
    "test_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "test_sentiment_desc = test_sentiment_desc.reset_index()\n",
    "test_sentiment_desc[\n",
    "    'sentiment_entities'] = test_sentiment_desc[\n",
    "    'sentiment_entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'sentiment'\n",
    "test_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\n",
    "for i in test_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\n",
    "test_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(aggregates)\n",
    "test_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in test_sentiment_gr.columns.tolist()])\n",
    "test_sentiment_gr = test_sentiment_gr.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "132f5680377aa571312a860adb1506cb05ef37b7"
   },
   "source": [
    "### merge processed DFs with base train/test DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "947ecbc24175147fb4808f7d85068a851d39cd57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 44) (3948, 43)\n"
     ]
    }
   ],
   "source": [
    "# Train merges:\n",
    "train_proc = train.copy()\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_gr, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_gr, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_desc, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "# Test merges:\n",
    "test_proc = test.copy()\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_desc, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "\n",
    "print(train_proc.shape, test_proc.shape)\n",
    "assert train_proc.shape[0] == train.shape[0]\n",
    "assert test_proc.shape[0] == test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc1c92dca3d8e7d3bee4e7c42c24def791e9df28"
   },
   "source": [
    "### add breed mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "a44dda1090936393134d00e5024b74c56935b4ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 48) (3948, 47)\n"
     ]
    }
   ],
   "source": [
    "train_breed_main = train_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "\n",
    "train_breed_main = train_breed_main.iloc[:, 2:]\n",
    "train_breed_main = train_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "train_breed_second = train_proc[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "\n",
    "train_breed_second = train_breed_second.iloc[:, 2:]\n",
    "train_breed_second = train_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "\n",
    "train_proc = pd.concat(\n",
    "    [train_proc, train_breed_main, train_breed_second], axis=1)\n",
    "\n",
    "\n",
    "test_breed_main = test_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "\n",
    "test_breed_main = test_breed_main.iloc[:, 2:]\n",
    "test_breed_main = test_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "test_breed_second = test_proc[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "\n",
    "test_breed_second = test_breed_second.iloc[:, 2:]\n",
    "test_breed_second = test_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "\n",
    "test_proc = pd.concat(\n",
    "    [test_proc, test_breed_main, test_breed_second], axis=1)\n",
    "\n",
    "print(train_proc.shape, test_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "509754ac9784439024c118860ceef27edbe4ce84"
   },
   "source": [
    "### concatenate train & test:\n",
    "\n",
    "Inspect NaN structure of the processed data:\n",
    "`AdoptionSpeed` is the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "a5561dc64fb58b47da60da90a8fa1aaa35349cce",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN structure:\n",
      "Type                                               0\n",
      "Name                                            1560\n",
      "Age                                                0\n",
      "Breed1                                             0\n",
      "Breed2                                             0\n",
      "Gender                                             0\n",
      "Color1                                             0\n",
      "Color2                                             0\n",
      "Color3                                             0\n",
      "MaturitySize                                       0\n",
      "FurLength                                          0\n",
      "Vaccinated                                         0\n",
      "Dewormed                                           0\n",
      "Sterilized                                         0\n",
      "Health                                             0\n",
      "Quantity                                           0\n",
      "Fee                                                0\n",
      "State                                              0\n",
      "RescuerID                                          0\n",
      "VideoAmt                                           0\n",
      "Description                                       14\n",
      "PetID                                              0\n",
      "PhotoAmt                                           0\n",
      "AdoptionSpeed                                   3948\n",
      "sentiment_sentiment_magnitude_MEAN               684\n",
      "sentiment_sentiment_magnitude_SUM                684\n",
      "sentiment_sentiment_score_MEAN                   684\n",
      "sentiment_sentiment_score_SUM                    684\n",
      "sentiment_sentiment_document_magnitude_MEAN      684\n",
      "sentiment_sentiment_document_magnitude_SUM       684\n",
      "sentiment_sentiment_document_score_MEAN          684\n",
      "sentiment_sentiment_document_score_SUM           684\n",
      "metadata_metadata_annots_score_MEAN              487\n",
      "metadata_metadata_annots_score_SUM               468\n",
      "metadata_metadata_color_score_MEAN               468\n",
      "metadata_metadata_color_score_SUM                468\n",
      "metadata_metadata_color_pixelfrac_MEAN           468\n",
      "metadata_metadata_color_pixelfrac_SUM            468\n",
      "metadata_metadata_crop_conf_MEAN                 468\n",
      "metadata_metadata_crop_conf_SUM                  468\n",
      "metadata_metadata_crop_importance_MEAN           468\n",
      "metadata_metadata_crop_importance_SUM            468\n",
      "metadata_annots_top_desc                         468\n",
      "sentiment_entities                               684\n",
      "main_breed_Type                                    5\n",
      "main_breed_BreedName                               5\n",
      "second_breed_Type                              13840\n",
      "second_breed_BreedName                         13840\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\n",
    "print('NaN structure:\\n{}'.format(np.sum(pd.isnull(X))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_uuid": "707623a2bc031be931b9793fa3a462f491e938cf"
   },
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(X, 'X_temp.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb503cf4d614ea3f7edd103f5d0cc570986ad0a2"
   },
   "source": [
    "### extract different column types:\n",
    "\n",
    "- integer columns are usually categorical features, which do not need encoding\n",
    "- float columns are numerical features\n",
    "- object columns are categorical features, which should be encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3528d7e8036c8b8bb0c5da254d9cd18d2019607d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tinteger columns:\n",
      "Type            int64\n",
      "Age             int64\n",
      "Breed1          int64\n",
      "Breed2          int64\n",
      "Gender          int64\n",
      "Color1          int64\n",
      "Color2          int64\n",
      "Color3          int64\n",
      "MaturitySize    int64\n",
      "FurLength       int64\n",
      "Vaccinated      int64\n",
      "Dewormed        int64\n",
      "Sterilized      int64\n",
      "Health          int64\n",
      "Quantity        int64\n",
      "Fee             int64\n",
      "State           int64\n",
      "VideoAmt        int64\n",
      "dtype: object\n",
      "\n",
      "\tfloat columns:\n",
      "PhotoAmt                                       float64\n",
      "AdoptionSpeed                                  float64\n",
      "sentiment_sentiment_magnitude_MEAN             float64\n",
      "sentiment_sentiment_magnitude_SUM              float64\n",
      "sentiment_sentiment_score_MEAN                 float64\n",
      "sentiment_sentiment_score_SUM                  float64\n",
      "sentiment_sentiment_document_magnitude_MEAN    float64\n",
      "sentiment_sentiment_document_magnitude_SUM     float64\n",
      "sentiment_sentiment_document_score_MEAN        float64\n",
      "sentiment_sentiment_document_score_SUM         float64\n",
      "metadata_metadata_annots_score_MEAN            float64\n",
      "metadata_metadata_annots_score_SUM             float64\n",
      "metadata_metadata_color_score_MEAN             float64\n",
      "metadata_metadata_color_score_SUM              float64\n",
      "metadata_metadata_color_pixelfrac_MEAN         float64\n",
      "metadata_metadata_color_pixelfrac_SUM          float64\n",
      "metadata_metadata_crop_conf_MEAN               float64\n",
      "metadata_metadata_crop_conf_SUM                float64\n",
      "metadata_metadata_crop_importance_MEAN         float64\n",
      "metadata_metadata_crop_importance_SUM          float64\n",
      "main_breed_Type                                float64\n",
      "second_breed_Type                              float64\n",
      "dtype: object\n",
      "\n",
      "\tto encode categorical columns:\n",
      "Name                        object\n",
      "RescuerID                   object\n",
      "Description                 object\n",
      "PetID                       object\n",
      "metadata_annots_top_desc    object\n",
      "sentiment_entities          object\n",
      "main_breed_BreedName        object\n",
      "second_breed_BreedName      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column_types = X.dtypes\n",
    "\n",
    "int_cols = column_types[column_types == 'int']\n",
    "float_cols = column_types[column_types == 'float']\n",
    "cat_cols = column_types[column_types == 'object']\n",
    "\n",
    "print('\\tinteger columns:\\n{}'.format(int_cols))\n",
    "print('\\n\\tfloat columns:\\n{}'.format(float_cols))\n",
    "print('\\n\\tto encode categorical columns:\\n{}'.format(cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a424da889496d55188c3c8478f0035c9fa8a7555"
   },
   "source": [
    "### feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "3a220d5a9cd973e97c894d57b1ef8f6c6e95fdc5"
   },
   "outputs": [],
   "source": [
    "# Copy original X DF for easier experimentation,\n",
    "# all feature engineering will be performed on this one:\n",
    "X_temp = X.copy()\n",
    "\n",
    "\n",
    "# Select subsets of columns:\n",
    "text_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\n",
    "categorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n",
    "\n",
    "# Names are all unique, so they can be dropped by default\n",
    "# Same goes for PetID, it shouldn't be used as a feature\n",
    "to_drop_columns = ['PetID', 'Name', 'RescuerID']\n",
    "# RescuerID will also be dropped, as a feature based on this column will be extracted independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "68d8f68c17adb43e1c5c86a773058b09c208d721",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count RescuerID occurrences:\n",
    "rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\n",
    "rescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n",
    "\n",
    "# Merge as another feature onto main DF:\n",
    "X_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "a228cc25a0b400527671a077471cde07068cc975"
   },
   "outputs": [],
   "source": [
    "# Factorize categorical columns:\n",
    "for i in categorical_columns:\n",
    "    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "c5e1043f2ba2ca95566ccfeb60ea04a470041c44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "# Subset text features:\n",
    "X_text = X_temp[text_columns]\n",
    "\n",
    "for i in X_text.columns:\n",
    "    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "0f5d0c82ad86bf1010cda2db2f35c1e32f4406fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features from: Description\n",
      "generating features from: metadata_annots_top_desc\n",
      "generating features from: sentiment_entities\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "\n",
    "n_components = 5\n",
    "text_features = []\n",
    "\n",
    "\n",
    "# Generate text features:\n",
    "for i in X_text.columns:\n",
    "    \n",
    "    # Initialize decomposition methods:\n",
    "    print('generating features from: {}'.format(i))\n",
    "    svd_ = TruncatedSVD(\n",
    "        n_components=n_components, random_state=1337)\n",
    "    nmf_ = NMF(\n",
    "        n_components=n_components, random_state=1337)\n",
    "    \n",
    "    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col)\n",
    "    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n",
    "    \n",
    "    nmf_col = nmf_.fit_transform(tfidf_col)\n",
    "    nmf_col = pd.DataFrame(nmf_col)\n",
    "    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n",
    "    \n",
    "    text_features.append(svd_col)\n",
    "    text_features.append(nmf_col)\n",
    "\n",
    "    \n",
    "# Combine all extracted features:\n",
    "text_features = pd.concat(text_features, axis=1)\n",
    "\n",
    "# Concatenate with main DF:\n",
    "X_temp = pd.concat([X_temp, text_features], axis=1)\n",
    "\n",
    "# Remove raw text columns:\n",
    "for i in X_text.columns:\n",
    "    X_temp = X_temp.drop(i, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "3ba80726173d872ca31e1c13e10256e05db0ff40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (18941, 73)\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns:\n",
    "X_temp = X_temp.drop(to_drop_columns, axis=1)\n",
    "\n",
    "# Check final df shape:\n",
    "print('X shape: {}'.format(X_temp.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5acc8411e12eb4f741956b1ae608017e10950040"
   },
   "source": [
    "### train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "ef5a8fdbe2b786bbc3ffdc797cfa3ca02262b70e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14993, 73)\n",
      "X_test shape: (3948, 72)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test again:\n",
    "X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "X_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "\n",
    "# Remove missing target column from test:\n",
    "X_test = X_test.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "\n",
    "assert X_train.shape[0] == train.shape[0]\n",
    "assert X_test.shape[0] == test.shape[0]\n",
    "\n",
    "\n",
    "# Check if columns between the two DFs are the same:\n",
    "train_cols = X_train.columns.tolist()\n",
    "train_cols.remove('AdoptionSpeed')\n",
    "\n",
    "test_cols = X_test.columns.tolist()\n",
    "\n",
    "assert np.all(train_cols == test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e0d21d076fc196facd083d6e3ae7cdbb1d575a3"
   },
   "source": [
    "### train and test NaN structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "0d232ef1ddd004597ac58f171697d122d17e63d8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                                               0\n",
       "Age                                                0\n",
       "Breed1                                             0\n",
       "Breed2                                             0\n",
       "Gender                                             0\n",
       "Color1                                             0\n",
       "Color2                                             0\n",
       "Color3                                             0\n",
       "MaturitySize                                       0\n",
       "FurLength                                          0\n",
       "Vaccinated                                         0\n",
       "Dewormed                                           0\n",
       "Sterilized                                         0\n",
       "Health                                             0\n",
       "Quantity                                           0\n",
       "Fee                                                0\n",
       "State                                              0\n",
       "VideoAmt                                           0\n",
       "PhotoAmt                                           0\n",
       "AdoptionSpeed                                      0\n",
       "sentiment_sentiment_magnitude_MEAN               551\n",
       "sentiment_sentiment_magnitude_SUM                551\n",
       "sentiment_sentiment_score_MEAN                   551\n",
       "sentiment_sentiment_score_SUM                    551\n",
       "sentiment_sentiment_document_magnitude_MEAN      551\n",
       "sentiment_sentiment_document_magnitude_SUM       551\n",
       "sentiment_sentiment_document_score_MEAN          551\n",
       "sentiment_sentiment_document_score_SUM           551\n",
       "metadata_metadata_annots_score_MEAN              356\n",
       "metadata_metadata_annots_score_SUM               341\n",
       "metadata_metadata_color_score_MEAN               341\n",
       "metadata_metadata_color_score_SUM                341\n",
       "metadata_metadata_color_pixelfrac_MEAN           341\n",
       "metadata_metadata_color_pixelfrac_SUM            341\n",
       "metadata_metadata_crop_conf_MEAN                 341\n",
       "metadata_metadata_crop_conf_SUM                  341\n",
       "metadata_metadata_crop_importance_MEAN           341\n",
       "metadata_metadata_crop_importance_SUM            341\n",
       "main_breed_Type                                    5\n",
       "main_breed_BreedName                               0\n",
       "second_breed_Type                              10762\n",
       "second_breed_BreedName                             0\n",
       "RescuerID_COUNT                                    0\n",
       "SVD_Description_0                                  0\n",
       "SVD_Description_1                                  0\n",
       "SVD_Description_2                                  0\n",
       "SVD_Description_3                                  0\n",
       "SVD_Description_4                                  0\n",
       "NMF_Description_0                                  0\n",
       "NMF_Description_1                                  0\n",
       "NMF_Description_2                                  0\n",
       "NMF_Description_3                                  0\n",
       "NMF_Description_4                                  0\n",
       "SVD_metadata_annots_top_desc_0                     0\n",
       "SVD_metadata_annots_top_desc_1                     0\n",
       "SVD_metadata_annots_top_desc_2                     0\n",
       "SVD_metadata_annots_top_desc_3                     0\n",
       "SVD_metadata_annots_top_desc_4                     0\n",
       "NMF_metadata_annots_top_desc_0                     0\n",
       "NMF_metadata_annots_top_desc_1                     0\n",
       "NMF_metadata_annots_top_desc_2                     0\n",
       "NMF_metadata_annots_top_desc_3                     0\n",
       "NMF_metadata_annots_top_desc_4                     0\n",
       "SVD_sentiment_entities_0                           0\n",
       "SVD_sentiment_entities_1                           0\n",
       "SVD_sentiment_entities_2                           0\n",
       "SVD_sentiment_entities_3                           0\n",
       "SVD_sentiment_entities_4                           0\n",
       "NMF_sentiment_entities_0                           0\n",
       "NMF_sentiment_entities_1                           0\n",
       "NMF_sentiment_entities_2                           0\n",
       "NMF_sentiment_entities_3                           0\n",
       "NMF_sentiment_entities_4                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pd.isnull(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "ec6ee11a3b7d0ca97d3faf0cccd06bab9dd17637",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                                              0\n",
       "Age                                               0\n",
       "Breed1                                            0\n",
       "Breed2                                            0\n",
       "Gender                                            0\n",
       "Color1                                            0\n",
       "Color2                                            0\n",
       "Color3                                            0\n",
       "MaturitySize                                      0\n",
       "FurLength                                         0\n",
       "Vaccinated                                        0\n",
       "Dewormed                                          0\n",
       "Sterilized                                        0\n",
       "Health                                            0\n",
       "Quantity                                          0\n",
       "Fee                                               0\n",
       "State                                             0\n",
       "VideoAmt                                          0\n",
       "PhotoAmt                                          0\n",
       "sentiment_sentiment_magnitude_MEAN              133\n",
       "sentiment_sentiment_magnitude_SUM               133\n",
       "sentiment_sentiment_score_MEAN                  133\n",
       "sentiment_sentiment_score_SUM                   133\n",
       "sentiment_sentiment_document_magnitude_MEAN     133\n",
       "sentiment_sentiment_document_magnitude_SUM      133\n",
       "sentiment_sentiment_document_score_MEAN         133\n",
       "sentiment_sentiment_document_score_SUM          133\n",
       "metadata_metadata_annots_score_MEAN             131\n",
       "metadata_metadata_annots_score_SUM              127\n",
       "metadata_metadata_color_score_MEAN              127\n",
       "metadata_metadata_color_score_SUM               127\n",
       "metadata_metadata_color_pixelfrac_MEAN          127\n",
       "metadata_metadata_color_pixelfrac_SUM           127\n",
       "metadata_metadata_crop_conf_MEAN                127\n",
       "metadata_metadata_crop_conf_SUM                 127\n",
       "metadata_metadata_crop_importance_MEAN          127\n",
       "metadata_metadata_crop_importance_SUM           127\n",
       "main_breed_Type                                   0\n",
       "main_breed_BreedName                              0\n",
       "second_breed_Type                              3078\n",
       "second_breed_BreedName                            0\n",
       "RescuerID_COUNT                                   0\n",
       "SVD_Description_0                                 0\n",
       "SVD_Description_1                                 0\n",
       "SVD_Description_2                                 0\n",
       "SVD_Description_3                                 0\n",
       "SVD_Description_4                                 0\n",
       "NMF_Description_0                                 0\n",
       "NMF_Description_1                                 0\n",
       "NMF_Description_2                                 0\n",
       "NMF_Description_3                                 0\n",
       "NMF_Description_4                                 0\n",
       "SVD_metadata_annots_top_desc_0                    0\n",
       "SVD_metadata_annots_top_desc_1                    0\n",
       "SVD_metadata_annots_top_desc_2                    0\n",
       "SVD_metadata_annots_top_desc_3                    0\n",
       "SVD_metadata_annots_top_desc_4                    0\n",
       "NMF_metadata_annots_top_desc_0                    0\n",
       "NMF_metadata_annots_top_desc_1                    0\n",
       "NMF_metadata_annots_top_desc_2                    0\n",
       "NMF_metadata_annots_top_desc_3                    0\n",
       "NMF_metadata_annots_top_desc_4                    0\n",
       "SVD_sentiment_entities_0                          0\n",
       "SVD_sentiment_entities_1                          0\n",
       "SVD_sentiment_entities_2                          0\n",
       "SVD_sentiment_entities_3                          0\n",
       "SVD_sentiment_entities_4                          0\n",
       "NMF_sentiment_entities_0                          0\n",
       "NMF_sentiment_entities_1                          0\n",
       "NMF_sentiment_entities_2                          0\n",
       "NMF_sentiment_entities_3                          0\n",
       "NMF_sentiment_entities_4                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pd.isnull(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a87c86df9fd01e6c1b744a7d21b40d816ca66106"
   },
   "source": [
    "### model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "7302b59080f81bde834d10e386dd35cdad394e12"
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "\n",
    "# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n",
    "\n",
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "af567e8fd28c1ce85578d21427e9c5936ad8e2e0"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.8,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 17}\n",
    "\n",
    "# Additional parameters:\n",
    "early_stop = 500\n",
    "verbose_eval = 100\n",
    "num_rounds = 10000\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "58ab4de93b0345c8c251e81f7de173de375a7953",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_tr distribution: Counter({4.0: 3357, 2.0: 3229, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.05986\tvalid_1's rmse: 1.09492\n",
      "[200]\ttraining's rmse: 1.00911\tvalid_1's rmse: 1.07014\n",
      "[300]\ttraining's rmse: 0.976339\tvalid_1's rmse: 1.05936\n",
      "[400]\ttraining's rmse: 0.952816\tvalid_1's rmse: 1.05245\n",
      "[500]\ttraining's rmse: 0.933527\tvalid_1's rmse: 1.04798\n",
      "[600]\ttraining's rmse: 0.919828\tvalid_1's rmse: 1.04573\n",
      "[700]\ttraining's rmse: 0.903831\tvalid_1's rmse: 1.04356\n",
      "[800]\ttraining's rmse: 0.888538\tvalid_1's rmse: 1.04206\n",
      "[900]\ttraining's rmse: 0.876047\tvalid_1's rmse: 1.0406\n",
      "[1000]\ttraining's rmse: 0.864823\tvalid_1's rmse: 1.03964\n",
      "[1100]\ttraining's rmse: 0.853846\tvalid_1's rmse: 1.03915\n",
      "[1200]\ttraining's rmse: 0.843928\tvalid_1's rmse: 1.03867\n",
      "[1300]\ttraining's rmse: 0.833349\tvalid_1's rmse: 1.03792\n",
      "[1400]\ttraining's rmse: 0.823992\tvalid_1's rmse: 1.03779\n",
      "[1500]\ttraining's rmse: 0.81362\tvalid_1's rmse: 1.03748\n",
      "[1600]\ttraining's rmse: 0.804822\tvalid_1's rmse: 1.03734\n",
      "[1700]\ttraining's rmse: 0.795937\tvalid_1's rmse: 1.037\n",
      "[1800]\ttraining's rmse: 0.786865\tvalid_1's rmse: 1.03705\n",
      "[1900]\ttraining's rmse: 0.780399\tvalid_1's rmse: 1.0374\n",
      "[2000]\ttraining's rmse: 0.772242\tvalid_1's rmse: 1.03776\n",
      "[2100]\ttraining's rmse: 0.764642\tvalid_1's rmse: 1.03794\n",
      "Early stopping, best iteration is:\n",
      "[1670]\ttraining's rmse: 0.798574\tvalid_1's rmse: 1.03693\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3357, 2.0: 3229, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.06231\tvalid_1's rmse: 1.09316\n",
      "[200]\ttraining's rmse: 1.01169\tvalid_1's rmse: 1.07127\n",
      "[300]\ttraining's rmse: 0.976707\tvalid_1's rmse: 1.06264\n",
      "[400]\ttraining's rmse: 0.952812\tvalid_1's rmse: 1.05949\n",
      "[500]\ttraining's rmse: 0.934323\tvalid_1's rmse: 1.05758\n",
      "[600]\ttraining's rmse: 0.919945\tvalid_1's rmse: 1.0563\n",
      "[700]\ttraining's rmse: 0.906103\tvalid_1's rmse: 1.05535\n",
      "[800]\ttraining's rmse: 0.892306\tvalid_1's rmse: 1.05491\n",
      "[900]\ttraining's rmse: 0.879324\tvalid_1's rmse: 1.05457\n",
      "[1000]\ttraining's rmse: 0.864191\tvalid_1's rmse: 1.05384\n",
      "[1100]\ttraining's rmse: 0.850509\tvalid_1's rmse: 1.05323\n",
      "[1200]\ttraining's rmse: 0.83914\tvalid_1's rmse: 1.05285\n",
      "[1300]\ttraining's rmse: 0.82684\tvalid_1's rmse: 1.05243\n",
      "[1400]\ttraining's rmse: 0.815235\tvalid_1's rmse: 1.05223\n",
      "[1500]\ttraining's rmse: 0.804563\tvalid_1's rmse: 1.05229\n",
      "[1600]\ttraining's rmse: 0.795844\tvalid_1's rmse: 1.05241\n",
      "[1700]\ttraining's rmse: 0.787281\tvalid_1's rmse: 1.05239\n",
      "[1800]\ttraining's rmse: 0.778642\tvalid_1's rmse: 1.05235\n",
      "[1900]\ttraining's rmse: 0.770188\tvalid_1's rmse: 1.05268\n",
      "Early stopping, best iteration is:\n",
      "[1448]\ttraining's rmse: 0.809894\tvalid_1's rmse: 1.05214\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.06325\tvalid_1's rmse: 1.08676\n",
      "[200]\ttraining's rmse: 1.01148\tvalid_1's rmse: 1.06073\n",
      "[300]\ttraining's rmse: 0.97966\tvalid_1's rmse: 1.051\n",
      "[400]\ttraining's rmse: 0.9553\tvalid_1's rmse: 1.04654\n",
      "[500]\ttraining's rmse: 0.937386\tvalid_1's rmse: 1.04422\n",
      "[600]\ttraining's rmse: 0.920958\tvalid_1's rmse: 1.04283\n",
      "[700]\ttraining's rmse: 0.905574\tvalid_1's rmse: 1.04174\n",
      "[800]\ttraining's rmse: 0.89174\tvalid_1's rmse: 1.0408\n",
      "[900]\ttraining's rmse: 0.878318\tvalid_1's rmse: 1.03979\n",
      "[1000]\ttraining's rmse: 0.866843\tvalid_1's rmse: 1.03913\n",
      "[1100]\ttraining's rmse: 0.853876\tvalid_1's rmse: 1.03862\n",
      "[1200]\ttraining's rmse: 0.842627\tvalid_1's rmse: 1.03881\n",
      "[1300]\ttraining's rmse: 0.830971\tvalid_1's rmse: 1.03914\n",
      "[1400]\ttraining's rmse: 0.819954\tvalid_1's rmse: 1.03964\n",
      "[1500]\ttraining's rmse: 0.809684\tvalid_1's rmse: 1.04006\n",
      "[1600]\ttraining's rmse: 0.798386\tvalid_1's rmse: 1.04015\n",
      "Early stopping, best iteration is:\n",
      "[1109]\ttraining's rmse: 0.852451\tvalid_1's rmse: 1.03856\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.0619\tvalid_1's rmse: 1.09236\n",
      "[200]\ttraining's rmse: 1.01097\tvalid_1's rmse: 1.06986\n",
      "[300]\ttraining's rmse: 0.978221\tvalid_1's rmse: 1.05987\n",
      "[400]\ttraining's rmse: 0.953137\tvalid_1's rmse: 1.05429\n",
      "[500]\ttraining's rmse: 0.932238\tvalid_1's rmse: 1.0511\n",
      "[600]\ttraining's rmse: 0.915652\tvalid_1's rmse: 1.04909\n",
      "[700]\ttraining's rmse: 0.900969\tvalid_1's rmse: 1.04736\n",
      "[800]\ttraining's rmse: 0.8884\tvalid_1's rmse: 1.04609\n",
      "[900]\ttraining's rmse: 0.875571\tvalid_1's rmse: 1.04558\n",
      "[1000]\ttraining's rmse: 0.863694\tvalid_1's rmse: 1.04507\n",
      "[1100]\ttraining's rmse: 0.852125\tvalid_1's rmse: 1.04444\n",
      "[1200]\ttraining's rmse: 0.842302\tvalid_1's rmse: 1.0439\n",
      "[1300]\ttraining's rmse: 0.833707\tvalid_1's rmse: 1.04391\n",
      "[1400]\ttraining's rmse: 0.823338\tvalid_1's rmse: 1.04382\n",
      "[1500]\ttraining's rmse: 0.813154\tvalid_1's rmse: 1.0436\n",
      "[1600]\ttraining's rmse: 0.803333\tvalid_1's rmse: 1.0438\n",
      "[1700]\ttraining's rmse: 0.79539\tvalid_1's rmse: 1.04418\n",
      "[1800]\ttraining's rmse: 0.786419\tvalid_1's rmse: 1.04482\n",
      "[1900]\ttraining's rmse: 0.778197\tvalid_1's rmse: 1.04499\n",
      "[2000]\ttraining's rmse: 0.76815\tvalid_1's rmse: 1.04523\n",
      "Early stopping, best iteration is:\n",
      "[1500]\ttraining's rmse: 0.813154\tvalid_1's rmse: 1.0436\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2608, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.06081\tvalid_1's rmse: 1.09678\n",
      "[200]\ttraining's rmse: 1.00976\tvalid_1's rmse: 1.07418\n",
      "[300]\ttraining's rmse: 0.977287\tvalid_1's rmse: 1.06361\n",
      "[400]\ttraining's rmse: 0.953835\tvalid_1's rmse: 1.05756\n",
      "[500]\ttraining's rmse: 0.936459\tvalid_1's rmse: 1.05421\n",
      "[600]\ttraining's rmse: 0.920607\tvalid_1's rmse: 1.05169\n",
      "[700]\ttraining's rmse: 0.904527\tvalid_1's rmse: 1.04981\n",
      "[800]\ttraining's rmse: 0.889683\tvalid_1's rmse: 1.04842\n",
      "[900]\ttraining's rmse: 0.874231\tvalid_1's rmse: 1.04756\n",
      "[1000]\ttraining's rmse: 0.860135\tvalid_1's rmse: 1.04704\n",
      "[1100]\ttraining's rmse: 0.847401\tvalid_1's rmse: 1.04668\n",
      "[1200]\ttraining's rmse: 0.836565\tvalid_1's rmse: 1.04625\n",
      "[1300]\ttraining's rmse: 0.827153\tvalid_1's rmse: 1.04593\n",
      "[1400]\ttraining's rmse: 0.817278\tvalid_1's rmse: 1.04562\n",
      "[1500]\ttraining's rmse: 0.80775\tvalid_1's rmse: 1.04577\n",
      "[1600]\ttraining's rmse: 0.797086\tvalid_1's rmse: 1.04542\n",
      "[1700]\ttraining's rmse: 0.788732\tvalid_1's rmse: 1.04541\n",
      "[1800]\ttraining's rmse: 0.780712\tvalid_1's rmse: 1.04559\n",
      "[1900]\ttraining's rmse: 0.773644\tvalid_1's rmse: 1.0459\n",
      "[2000]\ttraining's rmse: 0.765349\tvalid_1's rmse: 1.04595\n",
      "[2100]\ttraining's rmse: 0.758104\tvalid_1's rmse: 1.046\n",
      "Early stopping, best iteration is:\n",
      "[1648]\ttraining's rmse: 0.793304\tvalid_1's rmse: 1.04519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_splits, random_state=1337)\n",
    "\n",
    "\n",
    "oof_train = np.zeros((X_train.shape[0]))\n",
    "oof_test = np.zeros((X_test.shape[0], n_splits))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, valid_index in kfold.split(X_train, X_train['AdoptionSpeed'].values):\n",
    "    \n",
    "    X_tr = X_train.iloc[train_index, :]\n",
    "    X_val = X_train.iloc[valid_index, :]\n",
    "    \n",
    "    y_tr = X_tr['AdoptionSpeed'].values\n",
    "    X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n",
    "    \n",
    "    y_val = X_val['AdoptionSpeed'].values\n",
    "    X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n",
    "    \n",
    "    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n",
    "    \n",
    "    d_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    d_valid = lgb.Dataset(X_val, label=y_val)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    \n",
    "    print('training LGB:')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    \n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    oof_train[valid_index] = val_pred\n",
    "    oof_test[:, i] = test_pred\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "6d645345cb8ab39676376d10f6ce5aa0bc58d369"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  15.,  179., 1201., 2988., 3644., 3213., 2230., 1150.,  352.,\n",
       "          21.]),\n",
       " array([0.7081393 , 1.07385415, 1.43956901, 1.80528387, 2.17099873,\n",
       "        2.53671358, 2.90242844, 3.2681433 , 3.63385815, 3.99957301,\n",
       "        4.36528787]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(oof_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18b9db420430b00b0a466cbde7b662ada81a95cd"
   },
   "source": [
    "### adapted from: https://www.kaggle.com/myltykritik/simple-lgbm-image-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "476da82fb0a09d8ae23afb03859e05b70a87b44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Counts =  Counter({4.0: 4197, 2.0: 4037, 3.0: 3259, 1.0: 3090, 0.0: 410})\n",
      "Predicted Counts =  Counter({1.0: 4342, 4.0: 4178, 2.0: 3395, 3.0: 3078})\n",
      "Coefficients =  [0.50778428 2.16671439 2.50435565 2.84772716]\n",
      "QWK =  0.4430283050352751\n"
     ]
    }
   ],
   "source": [
    "# Compute QWK based on OOF train predictions:\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(oof_train, X_train['AdoptionSpeed'].values)\n",
    "coefficients = optR.coefficients()\n",
    "pred_test_y_k = optR.predict(oof_train, coefficients)\n",
    "print(\"\\nValid Counts = \", Counter(X_train['AdoptionSpeed'].values))\n",
    "print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "print(\"Coefficients = \", coefficients)\n",
    "qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, pred_test_y_k)\n",
    "print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "d2ccebb74d117385e00ab3d0a0df008f2c430b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pred distribution: Counter({4: 4239, 2: 3864, 1: 3227, 3: 3017, 0: 646})\n",
      "test pred distribution: Counter({4.0: 1172, 2.0: 999, 1.0: 874, 3.0: 765, 0.0: 138})\n"
     ]
    }
   ],
   "source": [
    "# Manually adjusted coefficients:\n",
    "\n",
    "coefficients_ = coefficients.copy()\n",
    "\n",
    "coefficients_[0] = 1.645\n",
    "coefficients_[1] = 2.115\n",
    "coefficients_[3] = 2.84\n",
    "\n",
    "train_predictions = optR.predict(oof_train, coefficients_).astype(int)\n",
    "print('train pred distribution: {}'.format(Counter(train_predictions)))\n",
    "\n",
    "test_predictions = optR.predict(oof_test.mean(axis=1), coefficients_)\n",
    "print('test pred distribution: {}'.format(Counter(test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "03ded9b8f81d756c3dc128d0d9a517d36a30c073",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Distribution:\n",
      "0.0    0.027346\n",
      "1.0    0.206096\n",
      "2.0    0.269259\n",
      "3.0    0.217368\n",
      "4.0    0.279931\n",
      "Name: AdoptionSpeed, dtype: float64\n",
      "\n",
      "Train Predicted Distribution:\n",
      "0    0.043087\n",
      "1    0.215234\n",
      "2    0.257720\n",
      "3    0.201227\n",
      "4    0.282732\n",
      "dtype: float64\n",
      "\n",
      "Test Predicted Distribution:\n",
      "0.0    0.034954\n",
      "1.0    0.221378\n",
      "2.0    0.253040\n",
      "3.0    0.193769\n",
      "4.0    0.296859\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Distribution inspection of original target and predicted train and test:\n",
    "\n",
    "print(\"True Distribution:\")\n",
    "print(pd.value_counts(X_train['AdoptionSpeed'], normalize=True).sort_index())\n",
    "print(\"\\nTrain Predicted Distribution:\")\n",
    "print(pd.value_counts(train_predictions, normalize=True).sort_index())\n",
    "print(\"\\nTest Predicted Distribution:\")\n",
    "print(pd.value_counts(test_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "49f62f5776c7f47f561541a928231d750d09d3ce"
   },
   "outputs": [],
   "source": [
    "# Generate submission:\n",
    "\n",
    "submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions.astype(np.int32)})\n",
    "submission.head()\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "bf15d791966aebc7f6788e33dca3c45a7de8e765"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
